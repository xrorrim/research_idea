# bignas：without any extra post-processing.

- “all child models are trained in a way such that they simultaneously reach excellent performance at the end of the search phase”

## Method :
- train in 1 stage
  - sandwich rule: smallest \ biggest and random 2
  - inplace distillation: biggest model supervise smaller models all the training
  - sample : first to 224, then interpolation
  - learning rate lower - stabler - less accurate
  - **initialization 0**
  - ***constant ended expo lr** (avoid large model overfitting)
  - batch normalization after training｜only norm whole model
- coarse to fine sample or slicing
  - two stage searching
  
| Stage | Operator | Resolution               | #Channels     | #Layers | Kernel Sizes |
|-------|----------|--------------------------|---------------|---------|--------------|
|       | Conv     | 192 × 192 - 320 × 320    | 32 - 40       | 1       | 3            |
| 1     | MBConv1  | 96 × 96 - 160 × 160      | 16 - 24       | 1 - 2   | 3            |
| 2     | MBConv6  | 96 × 96 - 160 × 160      | 24 - 32       | 2 - 3   | 3            |
| 3     | MBConv6  | 48 × 48 - 80 × 80        | 40 - 48       | 2 - 3   | 3, 5         |
| 4     | MBConv6  | 24 × 24 - 40 × 40        | 80 - 88       | 2 - 4   | 3, 5         |
| 5     | MBConv6  | 12 × 12 - 20 × 20        | 112 - 128     | 2 - 6   | 3, 5         |
| 6     | MBConv6  | 12 × 12 - 20 × 20        | 192 - 216     | 2 - 6   | 3, 5         |
| 7     | MBConv6  | 6 × 6 - 10 × 10          | 320 - 352     | 1 - 2   | 3, 5         |
|       | Conv     | 6 × 6 - 10 × 10          | 1280 - 1408   | 1       | 1            |

# AmoebaLLM：

- DP on depth; importance on width
  - DP
  - importance: norm in layer, sort in whole model
- SMoL: shape
- Finetune
  - Norm on T1
  - loss func (based on T1)
  - ```math
    L_{\text{total}} = L_{\text{CE}}(T_1(x), y) + \sum_{i=2}^K \frac{\|L_{\text{CE}}(T_1(x), y)\|}{\|L_{\text{CE}}(T_i(x), T_1(x))\|} L_{\text{CE}}(T_i(x), T_1(x))
    ```


# LolCATs: Low-Rank Linearizing

## Contribution

  - transfer from pretrained model
  - lora linearizing
  - scalability
## Method
  - Attention transfer
  - LoRA on linear weight ( delta(W) = BA )
  - 


