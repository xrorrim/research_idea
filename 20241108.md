# Bignas：without any extra post-processing.

- “all child models are trained in a way such that they simultaneously reach excellent performance at the end of the search phase”

## Method :
- train in 1 stage
  - sandwich rule: smallest \ biggest and random 2
  - inplace distillation: biggest model supervise smaller models all the training
  - sample : first to 224, then interpolation
  - learning rate lower - stabler - less accurate
  - **initialization 0**
  - ***constant ended expo lr** (avoid large model overfitting)
  - batch normalization after training｜only norm whole model
- coarse to fine sample or slicing
  - two stage searching
  
| Stage | Operator | Resolution               | #Channels     | #Layers | Kernel Sizes |
|-------|----------|--------------------------|---------------|---------|--------------|
|       | Conv     | 192 × 192 - 320 × 320    | 32 - 40       | 1       | 3            |
| 1     | MBConv1  | 96 × 96 - 160 × 160      | 16 - 24       | 1 - 2   | 3            |
| 2     | MBConv6  | 96 × 96 - 160 × 160      | 24 - 32       | 2 - 3   | 3            |
| 3     | MBConv6  | 48 × 48 - 80 × 80        | 40 - 48       | 2 - 3   | 3, 5         |
| 4     | MBConv6  | 24 × 24 - 40 × 40        | 80 - 88       | 2 - 4   | 3, 5         |
| 5     | MBConv6  | 12 × 12 - 20 × 20        | 112 - 128     | 2 - 6   | 3, 5         |
| 6     | MBConv6  | 12 × 12 - 20 × 20        | 192 - 216     | 2 - 6   | 3, 5         |
| 7     | MBConv6  | 6 × 6 - 10 × 10          | 320 - 352     | 1 - 2   | 3, 5         |
|       | Conv     | 6 × 6 - 10 × 10          | 1280 - 1408   | 1       | 1            |

# AmoebaLLM：

- DP on depth; importance on width
  - DP
  - importance: norm in layer, sort in whole model
- SMoL: shape
- Finetune
  - Norm on T1
  - loss func (based on T1)
  - ```math
    L_{\text{total}} = L_{\text{CE}}(T_1(x), y) + \sum_{i=2}^K \frac{\|L_{\text{CE}}(T_1(x), y)\|}{\|L_{\text{CE}}(T_i(x), T_1(x))\|} L_{\text{CE}}(T_i(x), T_1(x))
    ```


# LolCATs: Low-Rank Linearizing

- https://arxiv.org/abs/2410.10254
## Contribution

  - transfer from pretrained model
  - lora linearizing
  - scalability
## Method
  - Attention transfer
  - LoRA on linear weight ( delta(W) = BA )
## Shorts
* Efficiency Trade Off:
    * The test results provided by lolcats indicate that it only shows a significant advantage at extremely high batch sizes.
    * ![alt text](https://i.ibb.co/MS0BWPG/20241108-andrea-2.png "image title")

* Enhanced Long-Context Prediction
    * With Lolcats' low MMLU (Massive Multitask Language Understanding) score, there is room to improve performance in long-context understanding.* 
    * ![alt text](https://i.ibb.co/hgwPYXX/20241108-andrea-1.png "image title")
* Increased Time Efficiency
    * Currently, Lolcats' linear attention has throughput that is 10 times lower than the original softmax attention, indicating potential gains from optimizing this aspect.


# Duo Attention
- retrival heads and streaming heads (different kv cache policy)
- https://arxiv.org/abs/2410.10819
## Method:
  - optimize output on N*H scores
  - long context dataset
  - Loss function:
  
$$
\mathcal{L}_{\text{distill}} = \frac{1}{N} \sum_{i=1}^{N} \sum_{j=T-l+1}^{T} \left( H_{\text{full}}^{(i)}[j] - H_{\text{mixed}}^{(i)}[j] \right)^2
$$
$$
\mathcal{L}_{\text{reg}} = \sum_{i=1}^{L} \sum_{j=1}^{H} |\alpha_{i,j}|
$$
$$
\mathcal{L} = \mathcal{L}_{\text{distill}} + \lambda \mathcal{L}_{\text{reg}}
$$

# MInference
- apply different parterns on heads
## Method: 
- A shape, Vertical shape and Block
- offline determine partern of head
- online decide detailed parameters
  


