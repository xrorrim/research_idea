# learnable adam

learn a block-wide learning rate on adam optimizer

loss(W) -> g

W' = W - ag

# related work
- MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging
  - https://arxiv.org/abs/2206.01408
- META-LEARNING WITH NEGATIVE LEARNING RATES
    - https://arxiv.org/pdf/2102.00940