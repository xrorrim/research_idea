# THINKING LLMS: GENERAL INSTRUCTION FOLLOWING WITH THOUGHT GENERATION
## method
  - "Here is my thinking proccess" and "Here is my response"
  - RL"AI"F training -- Direct Preference Optimization (DPO or IRPO as loss) 
  - "Best" and "Worst" response, then learn which thought is leading to correct answer
  - Length penalty

## ELO - eval k pair of evaluation

# Let’s Verify Step by Step (OPENAI)
# method
  - focus on Reward Model
  - insight: process supervision is better; larger is better
  - we deliberately choose to supervise only up to the first incorrect step
  - prm800k dataset on math process
    -  The PRM800K training set contains 800K step-level labels across 75K solutions to 12K problems.
    -  include data from 4.5K MATH test problems in the PRM800K 

# Fast Best-of-N
## method
  - reject before OOM
## strength
  - outperform computation consuming Best-of-N

# LLMonkeys:
  - used selection method: majority voting (seems good enough) and reward model scoring and combine them (score-weighed voting)
  - used model: ArmoRM
# ArmoRM : RLHFlow

# Generative Reward Models
  - hard (STaR and CoT and ……)
  
# Pairwise Ranking and Generative Fusion
  - Pair -> Rank Top K (multilayer 1-head) -> Mix by fuser (3B model)
# Bradley-Terry

# Fast Best-of-N on MATH
## Method
  - Regression (Multi-value score) and MoE (Gate func, predict importance of score)
## problem
  - even Best-of-N result is not ideal
    - ( which means baseline is low )
    -  normal RM plateaus around 100 samples, serving as a lower bound of our porject
  - MATH problems have "Ground Truth" compared with language task
  - One wrong step is enough to tell the result
  - (What if) not cutting down, generate more branches instead ?

